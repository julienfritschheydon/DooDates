# DooDates - Intégration IA Conversationnelle

**Document créé le 23 juin 2025**

## Vue d'Ensemble

### Objectif
Permettre aux utilisateurs de créer des sondages en langage naturel via une conversation avec un assistant IA, en utilisant Mistral AI pour le parsing et la génération automatique.

### Architecture IA
```
Frontend (React)
├── ChatInterface.tsx (interface utilisateur)
├── MessageStream.tsx (streaming responses)
└── ConversationContext.tsx (état global)

Backend API
├── /conversations (gestion conversations)
├── /ai/chat (endpoint principal IA)
└── /ai/functions (function calling)

Mistral AI
├── mistral-large-latest (modèle principal)
├── Function Calling (création automatique)
└── Streaming (réponses temps réel)

Context Management
├── Extraction d'informations
├── Mémoire conversationnelle
└── Gestion des clarifications
```

---

## Configuration Mistral AI

### Setup API
```typescript
// lib/mistral.ts
import { MistralApi } from '@mistralai/mistralai';

export const mistral = new MistralApi({
  apiKey: process.env.MISTRAL_API_KEY!,
});

export const MISTRAL_CONFIG = {
  model: 'mistral-large-latest',
  temperature: 0.1, // Réponses plus déterministes
  maxTokens: 1000,
  stream: true,
  safePrompt: true
} as const;
```

### Variables d'environnement
```env
# Mistral AI
MISTRAL_API_KEY=your_mistral_api_key_here
MISTRAL_MODEL=mistral-large-latest
MISTRAL_MAX_TOKENS=1000
MISTRAL_TEMPERATURE=0.1

# Fallback configuration
MISTRAL_FALLBACK_MODEL=mistral-medium
MISTRAL_MAX_RETRIES=3
MISTRAL_TIMEOUT_MS=30000
```

---

## Prompts Système

### Prompt Principal
```typescript
export const SYSTEM_PROMPT = `
Tu es DooDates AI, un assistant spécialisé dans la création de sondages de disponibilité.

## OBJECTIF
Aider l'utilisateur à créer un sondage en extrayant les informations de sa demande et en posant des questions de clarification si nécessaire.

## INFORMATIONS À EXTRAIRE
1. **Titre/objet** : Type de réunion, événement ou activité
2. **Dates proposées** : Minimum 1, maximum 30 dates
3. **Horaires** : Créneaux spécifiques ou journée entière
4. **Participants** : Emails si fournis, sinon demander
5. **Contexte** : Lieu, durée, type d'événement

## RÈGLES DE CONVERSATION
- Être concis et efficace
- Poser UNE question à la fois
- Confirmer les informations extraites avant création
- Proposer des alternatives intelligentes
- Utiliser un ton amical mais professionnel
- Toujours utiliser les functions pour créer le sondage

## GESTION DES DATES
- Interpréter "mardi prochain", "la semaine prochaine", etc.
- Proposer des dates précises (ex: "mardi 25 juin")
- Vérifier que les dates ne sont pas dans le passé
- Limiter à 30 dates maximum

## GESTION DES HORAIRES
- Si non spécifiés, proposer des créneaux classiques
- Respecter les fuseaux horaires (défaut: Europe/Paris)
- Proposer des granularités adaptées (30min, 1h, 2h)

## EXEMPLES DE CONVERSATIONS

### Exemple 1 : Demande simple
User: "Il faut qu'on organise une réunion équipe mardi ou mercredi prochain"
Assistant: "Parfait ! Pour la réunion équipe, je propose mardi 25 juin et mercredi 26 juin. Préférez-vous un créneau spécifique (ex: 14h-16h) ou toute la journée ?"

### Exemple 2 : Demande avec détails
User: "Dîner avec Paul et Marie samedi 29 ou dimanche 30 juin vers 19h"
Assistant: "Excellent ! Je crée un sondage 'Dîner avec Paul et Marie' pour samedi 29 et dimanche 30 juin vers 19h. Dois-je ajouter paul@email.com et marie@email.com comme participants ?"

### Exemple 3 : Demande incomplète
User: "Réunion la semaine prochaine"
Assistant: "Pour quelle réunion souhaitez-vous créer un sondage ? Et quels jours de la semaine prochaine vous conviendraient le mieux ?"

## FUNCTIONS DISPONIBLES
Utilise TOUJOURS les functions pour créer des sondages ou demander des clarifications.
`;
```

### Prompts Contextuels
```typescript
export const CONTEXT_PROMPTS = {
  dateExtraction: `
Extrait les dates mentionnées dans le message utilisateur.
Convertis les expressions relatives en dates absolues.
Aujourd'hui: {currentDate}
`,
  
  participantExtraction: `
Identifie les participants mentionnés (noms, emails).
Détecte les indices sur le nombre de participants.
`,
  
  timeExtraction: `
Extrait les créneaux horaires mentionnés.
Interpréte "matin", "après-midi", "soir" en créneaux précis.
Propose des granularités adaptées.
`
};
```

---

## Function Calling

### Définition des Functions
```typescript
// ai/functions.ts
export const MISTRAL_FUNCTIONS = [
  {
    type: "function",
    function: {
      name: "create_poll_draft",
      description: "Crée un brouillon de sondage à partir des informations extraites de la conversation",
      parameters: {
        type: "object",
        properties: {
          title: {
            type: "string",
            description: "Titre du sondage généré automatiquement à partir du contexte"
          },
          dates: {
            type: "array",
            items: { 
              type: "string", 
              format: "date",
              description: "Date au format YYYY-MM-DD"
            },
            description: "Liste des dates proposées extraites de la conversation",
            minItems: 1,
            maxItems: 30
          },
          timeSlots: {
            type: "object",
            description: "Créneaux horaires par date si mentionnés",
            patternProperties: {
              "^\\d{4}-\\d{2}-\\d{2}$": {
                type: "array",
                items: {
                  type: "object",
                  properties: {
                    start_hour: { type: "integer", minimum: 0, maximum: 23 },
                    start_minute: { type: "integer", minimum: 0, maximum: 59 },
                    end_hour: { type: "integer", minimum: 0, maximum: 23 },
                    end_minute: { type: "integer", minimum: 0, maximum: 59 },
                    label: { type: "string" }
                  },
                  required: ["start_hour", "start_minute", "end_hour", "end_minute"]
                }
              }
            }
          },
          participants: {
            type: "array",
            items: { 
              type: "string", 
              format: "email" 
            },
            description: "Emails des participants mentionnés dans la conversation"
          },
          description: {
            type: "string",
            description: "Description optionnelle du sondage"
          },
          context: {
            type: "object",
            description: "Contexte additionnel extrait",
            properties: {
              location: { type: "string" },
              duration: { type: "integer" },
              type: { 
                type: "string", 
                enum: ["meeting", "event", "meal", "activity", "other"] 
              },
              timezone: { type: "string", default: "Europe/Paris" }
            }
          }
        },
        required: ["title", "dates"]
      }
    }
  },
  {
    type: "function",
    function: {
      name: "ask_clarification",
      description: "Demande une clarification à l'utilisateur quand les informations sont incomplètes ou ambiguës",
      parameters: {
        type: "object",
        properties: {
          question: {
            type: "string",
            description: "Question précise à poser à l'utilisateur"
          },
          suggestions: {
            type: "array",
            items: { type: "string" },
            description: "Suggestions de réponses pour guider l'utilisateur",
            maxItems: 5
          },
          missing_info: {
            type: "string",
            enum: ["dates", "times", "participants", "title", "details"],
            description: "Type d'information manquante"
          }
        },
        required: ["question", "missing_info"]
      }
    }
  },
  {
    type: "function",
    function: {
      name: "update_conversation_context",
      description: "Met à jour le contexte de la conversation avec de nouvelles informations",
      parameters: {
        type: "object",
        properties: {
          extracted_info: {
            type: "object",
            description: "Informations mises à jour",
            properties: {
              title: { type: "string" },
              dates: { type: "array", items: { type: "string" } },
              participants: { type: "array", items: { type: "string" } },
              timeSlots: { type: "object" },
              context: { type: "object" }
            }
          },
          confidence_score: {
            type: "number",
            minimum: 0,
            maximum: 1,
            description: "Score de confiance dans l'extraction (0-1)"
          }
        },
        required: ["extracted_info"]
      }
    }
  }
];
```

### Implémentation des Functions
```typescript
// ai/functionHandlers.ts
export class FunctionHandler {
  async createPollDraft(args: CreatePollDraftArgs, conversationId: string) {
    try {
      // Validation des arguments
      const validated = CreatePollDraftSchema.parse(args);
      
      // Création du brouillon en base
      const poll = await supabase
        .from('polls')
        .insert({
          title: validated.title,
          description: validated.description,
          slug: await generateSlug(validated.title),
          status: 'draft',
          settings: {
            time_granularity: this.inferGranularity(validated.timeSlots),
            allow_anonymous_votes: true,
            allow_maybe_votes: true,
            send_notifications: true
          }
        })
        .select()
        .single();

      // Création des options
      await this.createPollOptions(poll.data.id, validated.dates, validated.timeSlots);
      
      // Mise à jour de la conversation
      await this.updateConversation(conversationId, {
        poll_id: poll.data.id,
        status: 'completed'
      });

      return {
        success: true,
        poll_id: poll.data.id,
        share_url: `https://doodates.app/poll/${poll.data.slug}`,
        message: "Sondage créé avec succès ! Vous pouvez maintenant l'envoyer aux participants."
      };
    } catch (error) {
      return {
        success: false,
        error: error.message,
        message: "Erreur lors de la création du sondage. Pouvez-vous vérifier les informations ?"
      };
    }
  }

  async askClarification(args: AskClarificationArgs, conversationId: string) {
    // Marquer dans le contexte qu'une clarification est en cours
    await this.updateConversationContext(conversationId, {
      clarification_pending: {
        question: args.question,
        missing_info: args.missing_info,
        timestamp: new Date().toISOString()
      }
    });

    return {
      question: args.question,
      suggestions: args.suggestions,
      requires_response: true
    };
  }

  async updateConversationContext(conversationId: string, updates: any) {
    const { data: conversation } = await supabase
      .from('conversations')
      .select('context')
      .eq('id', conversationId)
      .single();

    const updatedContext = {
      ...conversation?.context,
      ...updates,
      last_updated: new Date().toISOString()
    };

    await supabase
      .from('conversations')
      .update({ context: updatedContext })
      .eq('id', conversationId);

    return updatedContext;
  }

  private inferGranularity(timeSlots: any): number {
    // Logique pour inférer la granularité optimale
    if (!timeSlots || Object.keys(timeSlots).length === 0) {
      return 60; // 1 heure par défaut
    }

    // Analyser les créneaux pour déterminer la granularité
    const durations = Object.values(timeSlots).flat().map((slot: any) => {
      const start = slot.start_hour * 60 + slot.start_minute;
      const end = slot.end_hour * 60 + slot.end_minute;
      return end - start;
    });

    const avgDuration = durations.reduce((a, b) => a + b, 0) / durations.length;
    
    if (avgDuration <= 30) return 15;
    if (avgDuration <= 60) return 30;
    if (avgDuration <= 120) return 60;
    return 120;
  }
}
```

---

## Context Management

### Structure du Contexte
```typescript
// types/conversation.ts
interface ConversationContext {
  // Informations extraites
  extracted_info: {
    title?: string;
    dates: string[];
    participants: string[];
    timeSlots: Record<string, TimeSlot[]>;
    location?: string;
    duration?: number;
    type?: EventType;
    timezone: string;
  };
  
  // État de la conversation
  conversation_state: {
    phase: 'initial' | 'extracting' | 'clarifying' | 'confirming' | 'creating' | 'completed';
    clarifications_needed: string[];
    confidence_score: number;
    last_action: string;
  };
  
  // Historique des clarifications
  clarification_history: Array<{
    question: string;
    answer: string;
    timestamp: string;
    resolved: boolean;
  }>;
  
  // Métadonnées
  metadata: {
    language: string;
    user_timezone: string;
    session_start: string;
    last_updated: string;
  };
  
  // Brouillon du sondage
  poll_draft?: {
    id?: string;
    title: string;
    dates: string[];
    timeSlots: Record<string, TimeSlot[]>;
    participants: string[];
    settings: PollSettings;
  };
}
```

### Gestion de la Mémoire
```typescript
// services/contextManager.ts
export class ContextManager {
  private maxMessages = 20; // Limite mémoire courte
  private maxAge = 30 * 24 * 60 * 60 * 1000; // 30 jours
  
  async getConversationContext(conversationId: string): Promise<ConversationContext> {
    const { data } = await supabase
      .from('conversations')
      .select('context, messages')
      .eq('id', conversationId)
      .single();
    
    return data?.context || this.getDefaultContext();
  }
  
  async updateContext(conversationId: string, updates: Partial<ConversationContext>) {
    const current = await this.getConversationContext(conversationId);
    const updated = this.mergeContext(current, updates);
    
    await supabase
      .from('conversations')
      .update({ 
        context: updated,
        updated_at: new Date().toISOString()
      })
      .eq('id', conversationId);
    
    return updated;
  }
  
  private mergeContext(current: ConversationContext, updates: Partial<ConversationContext>): ConversationContext {
    return {
      ...current,
      extracted_info: {
        ...current.extracted_info,
        ...updates.extracted_info
      },
      conversation_state: {
        ...current.conversation_state,
        ...updates.conversation_state,
        last_action: updates.conversation_state?.phase || current.conversation_state.last_action
      },
      metadata: {
        ...current.metadata,
        last_updated: new Date().toISOString()
      }
    };
  }
  
  async cleanupOldConversations() {
    const cutoff = new Date(Date.now() - this.maxAge);
    
    await supabase
      .from('conversations')
      .delete()
      .lt('updated_at', cutoff.toISOString())
      .eq('status', 'abandoned');
  }
}
```

---

## Streaming Implementation

### Backend Streaming
```typescript
// api/ai/chat.ts
export async function POST(request: Request) {
  const { conversationId, message } = await request.json();
  
  // Configuration du stream
  const encoder = new TextEncoder();
  const stream = new ReadableStream({
    async start(controller) {
      try {
        // Récupération du contexte
        const context = await contextManager.getConversationContext(conversationId);
        
        // Préparation des messages pour Mistral
        const messages = await prepareMessages(conversationId, message, context);
        
        // Appel Mistral avec streaming
        const response = await mistral.chatStream({
          model: MISTRAL_CONFIG.model,
          messages,
          functions: MISTRAL_FUNCTIONS,
          temperature: MISTRAL_CONFIG.temperature,
          maxTokens: MISTRAL_CONFIG.maxTokens
        });
        
        let assistantMessage = '';
        let functionCall = null;
        
        for await (const chunk of response) {
          const delta = chunk.choices[0]?.delta;
          
          if (delta?.content) {
            assistantMessage += delta.content;
            
            // Envoyer le chunk au client
            controller.enqueue(encoder.encode(`data: ${JSON.stringify({
              type: 'content',
              content: delta.content
            })}\n\n`));
          }
          
          if (delta?.function_call) {
            functionCall = delta.function_call;
          }
        }
        
        // Traitement des function calls
        if (functionCall) {
          const result = await handleFunctionCall(functionCall, conversationId);
          
          controller.enqueue(encoder.encode(`data: ${JSON.stringify({
            type: 'function_result',
            result
          })}\n\n`));
        }
        
        // Sauvegarde des messages
        await saveMessages(conversationId, message, assistantMessage, functionCall);
        
        controller.enqueue(encoder.encode('data: [DONE]\n\n'));
        controller.close();
        
      } catch (error) {
        controller.enqueue(encoder.encode(`data: ${JSON.stringify({
          type: 'error',
          error: error.message
        })}\n\n`));
        controller.close();
      }
    }
  });
  
  return new Response(stream, {
    headers: {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive'
    }
  });
}
```

### Frontend Streaming
```typescript
// hooks/useAIChat.ts
export function useAIChat(conversationId: string) {
  const [messages, setMessages] = useState<Message[]>([]);
  const [isStreaming, setIsStreaming] = useState(false);
  const [currentResponse, setCurrentResponse] = useState('');
  
  const sendMessage = async (content: string) => {
    setIsStreaming(true);
    setCurrentResponse('');
    
    // Ajouter le message utilisateur
    const userMessage: Message = {
      id: generateId(),
      role: 'user',
      content,
      timestamp: new Date().toISOString()
    };
    setMessages(prev => [...prev, userMessage]);
    
    try {
      const response = await fetch('/api/v1/ai/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ conversationId, message: content })
      });
      
      const reader = response.body?.getReader();
      if (!reader) throw new Error('No reader available');
      
      let assistantContent = '';
      
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        
        const chunk = new TextDecoder().decode(value);
        const lines = chunk.split('\n').filter(line => line.startsWith('data: '));
        
        for (const line of lines) {
          const data = line.slice(6);
          if (data === '[DONE]') {
            setIsStreaming(false);
            break;
          }
          
          try {
            const parsed = JSON.parse(data);
            
            if (parsed.type === 'content') {
              assistantContent += parsed.content;
              setCurrentResponse(assistantContent);
            } else if (parsed.type === 'function_result') {
              // Traiter le résultat de la function
              handleFunctionResult(parsed.result);
            }
          } catch (e) {
            console.warn('Failed to parse SSE data:', data);
          }
        }
      }
      
      // Ajouter la réponse complète de l'assistant
      if (assistantContent) {
        const assistantMessage: Message = {
          id: generateId(),
          role: 'assistant',
          content: assistantContent,
          timestamp: new Date().toISOString()
        };
        setMessages(prev => [...prev, assistantMessage]);
      }
      
    } catch (error) {
      console.error('Chat error:', error);
      setIsStreaming(false);
    } finally {
      setCurrentResponse('');
    }
  };
  
  return {
    messages,
    isStreaming,
    currentResponse,
    sendMessage
  };
}
```

---

## Error Handling & Fallbacks

### Gestion des Erreurs Mistral
```typescript
// services/aiService.ts
export class AIService {
  private maxRetries = 3;
  private retryDelay = 1000;
  
  async chatWithRetry(messages: Message[], attempt = 1): Promise<any> {
    try {
      return await mistral.chat({
        model: MISTRAL_CONFIG.model,
        messages,
        functions: MISTRAL_FUNCTIONS
      });
    } catch (error) {
      if (attempt >= this.maxRetries) {
        throw new Error(`Mistral API failed after ${this.maxRetries} attempts: ${error.message}`);
      }
      
      // Retry avec backoff exponentiel
      await new Promise(resolve => setTimeout(resolve, this.retryDelay * attempt));
      return this.chatWithRetry(messages, attempt + 1);
    }
  }
  
  async fallbackToSimpleExtraction(userMessage: string): Promise<any> {
    // Extraction simple sans IA en cas d'échec
    const datePattern = /\d{1,2}\/\d{1,2}|\d{1,2}\s+(janvier|février|mars|avril|mai|juin|juillet|août|septembre|octobre|novembre|décembre)/gi;
    const emailPattern = /[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}/g;
    
    const dates = userMessage.match(datePattern) || [];
    const emails = userMessage.match(emailPattern) || [];
    
    return {
      title: "Nouveau sondage",
      dates: dates.slice(0, 5), // Limiter à 5 dates
      participants: emails,
      confidence: 0.3 // Faible confiance
    };
  }
}
```

### Monitoring et Analytics
```typescript
// services/aiAnalytics.ts
export class AIAnalytics {
  async trackConversation(conversationId: string, metrics: {
    messages_count: number;
    duration_ms: number;
    success: boolean;
    function_calls: number;
    errors: string[];
  }) {
    await supabase
      .from('analytics_events')
      .insert({
        event_type: 'ai_conversation_completed',
        event_data: {
          conversation_id: conversationId,
          ...metrics
        }
      });
  }
  
  async trackFunctionCall(functionName: string, success: boolean, executionTime: number) {
    await supabase
      .from('analytics_events')
      .insert({
        event_type: 'ai_function_called',
        event_data: {
          function_name: functionName,
          success,
          execution_time_ms: executionTime
        }
      });
  }
}
```

---

**Document créé le 23 juin 2025**  
**Status :** Spécifications IA complètes Phase 2  
**Prochaine étape :** Implémentation Mistral + Function Calling 